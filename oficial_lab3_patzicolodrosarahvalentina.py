# -*- coding: utf-8 -*-
"""Oficial Lab3-PatziColodroSarahValentina.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13NUUsyoi3C87c4f4aV8z6Sug4U6K-Q0r

# Ejercicion de programación - Regresión Logistica

En este ejercicio se implementa regresion logistica y se aplica a dos diferentes datasets.
"""

# Commented out IPython magic to ensure Python compatibility.
# se utiliza para el manejo de rutas y directorios.
import os

# para generar numeros aleatorios
import random

# Calculo cientifico y vectorial para python
import numpy as np

# Librerias para graficar
from matplotlib import pyplot

# Modulo de optimización de scipy
from scipy import optimize

# Biblioteca para la manipulación y el análisis de datos
import pandas as pd

# le dice a matplotlib que incruste gráficos en el cuaderno
# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

"""## 1 Regresion Logistica

En esta parte del ejercicio, creará un modelo de regresión logística para predecir si un estudiante será admitido en una universidad. Suponga que es el administrador de un departamento universitario y desea determinar las posibilidades de admisión de cada solicitante en función de sus resultados en dos exámenes. Tiene datos históricos de solicitantes anteriores que puede usar como un conjunto de capacitación para la regresión logística. Para cada ejemplo de capacitación, se tiene las calificaciones del solicitante en dos exámenes y la decisión de admisión. Su tarea es crear un modelo de clasificación que calcule la probabilidad de admisión de un solicitante en función de los puntajes de esos dos exámenes.

La siguiente celda cargará los datos y las etiquetas correspondientes:
"""

# Cargar datos
# Las dos primeras columnas contienen la nota de dos examenes y la tercera columna
# contiene la etiqueta que indica si el alumno ingreso o no a la universidad.
# data = np.loadtxt(os.path.join('data', 'ex2data1.txt'), delimiter=',')

# Cargamos el dataset
data = pd.read_csv('/content/drive/MyDrive/IA/LaboratoriosOficiales/Lab3-PatziColodroSarahValentina/american_bankruptcy.csv', delimiter=',')
#imprimir el data
#print(data.head())
display(data)
data.isnull().sum()

#limpiar la caracteristica status_label para que 'alive': 1, 'failed': 0 (alive = True = 1: Si algo está "vivo", failed = False = 0: Si algo ha "fallado")
data['status_label'] = data['status_label'].replace({'alive': 1, 'failed': 0})
#imprimir el data
display(data)
#mostrar informacion de cuantas filas de 0 y cuantas de 1 tiene status_label
print(data['status_label'].value_counts())

#eliminar la columna company_name y la del year
data = data.drop(['company_name', 'year'], axis=1)
#imprimir el data
display(data)

# Selección de las primeras 11 características (X1 a X11) y la variable objetivo es 'status_label'

#definir valor de X escogiendo por numero de posicion las que yo quiera poner
X = data.iloc[:30000,[1,2,3,4,5,6,7,8,9,10,11]]
y = data.iloc[:30000,0]

#imprimir la cantidad de filas y columnas que tiene el data
print('En X: ',X.shape)
print('En Y: ',y.shape)


#imprimir
print('Visualización de Columnas y Filas Usadas:')
print('{:>0s}{:>21s}{:>15s}{:>15s}{:>15s}{:>15s}{:>15s}{:>15s}{:>15s}{:>15s}{:>15s}{:>15s}'.format( 'y','X[:,0]','X[:,1]','X[:,2]','X[:,3]','X[:,4]','X[:,5]','X[:,6]','X[:,7]','X[:,8]','X[:,9]','X[:,10]')) # Imprime los nombres de las columnas
# Ajusta la cantidad de valores que imprimes dentro del loop para que coincida con la cantidad de columnas en X
for i in range(10):
    print('{:<15.0f}{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}'.format(
        y.iloc[i], X.iloc[i, 0], X.iloc[i, 1], X.iloc[i, 2], X.iloc[i, 3], X.iloc[i, 4], X.iloc[i, 5], X.iloc[i, 6], X.iloc[i, 7], X.iloc[i, 8],X.iloc[i, 9],X.iloc[i, 10]
    ))

#convertir X y Y en numpy
X = X.to_numpy()
y = y.to_numpy()

"""### **Cargamos variables distintas para guardar 100 filas posteriores a las usadas para el entrenamiento**

Se usa a partir de 30000
"""

# Seleccionamos las filas 17,000 a 17,100 para las características y la variable de salida
X_predic = data.iloc[30000:30100, [1,2,3,4,5,6,7,8,9,10,11]]  # Características de las filas adicionales
y_predic = data.iloc[30000:30100, 0]   # Variable de salida de las filas adicionales

print('Visualización de Columnas y Filas Usadas:')
print('{:>0s}{:>21s}{:>15s}{:>15s}{:>15s}{:>15s}{:>15s}{:>15s}{:>15s}{:>15s}{:>15s}{:>15s}'.format( 'y','X[:,0]','X[:,1]','X[:,2]','X[:,3]','X[:,4]','X[:,5]','X[:,6]','X[:,7]','X[:,8]','X[:,9]','X[:,10]')) # Imprime los nombres de las columnas
# Ajusta la cantidad de valores que imprimes dentro del loop para que coincida con la cantidad de columnas en X
for i in range(10):
    print('{:<15.0f}{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}{:<15.3f}'.format(
        y_predic.iloc[i], X_predic.iloc[i, 0], X_predic.iloc[i, 1], X_predic.iloc[i, 2], X_predic.iloc[i, 3], X_predic.iloc[i, 4], X_predic.iloc[i, 5], X_predic.iloc[i, 6], X_predic.iloc[i, 7], X_predic.iloc[i, 8],X_predic.iloc[i, 9],X_predic.iloc[i, 10]
    ))

#imprimir la cantidad de filas y columnas que tiene el data
print('En X: ',X_predic.shape)
print('En Y: ',y_predic.shape)

"""### **Normalizar Caracteristicas**
 Este es un paso importante en el preprocesamiento de datos en aprendizaje automático, transformamos las caracteristicas a valores similares para hacer que converjan mas rápido
"""

def  featureNormalize(X):
    X_norm = X.copy()
    mu = np.zeros(X.shape[1])
    sigma = np.zeros(X.shape[1])

    mu = np.mean(X, axis = 0)
    sigma = np.std(X, axis = 0)
    X_norm = (X - mu) / sigma

    return X_norm, mu, sigma

# Llama a la funcion featureNormalize con el parametro X para obtener los datos normalizados
X_norm, mu, sigma = featureNormalize(X)

print('Media calculada:\n', mu)
print('Desviación estandar calculada:\n', sigma)
print('Datos normalizados:\n',X_norm) # Muestra los datos normalizados

"""### Normalizar datos prediccion"""

def  featureNormalize(X_predic):
    X_norm_predic = X_predic.copy()
    mu = np.zeros(X_predic.shape[1])
    sigma = np.zeros(X_predic.shape[1])

    mu = np.mean(X_predic, axis = 0)
    sigma = np.std(X_predic, axis = 0)
    X_norm_predic = (X_predic - mu) / sigma

    return X_norm_predic, mu, sigma

# Llama a la funcion featureNormalize con el parametro X para obtener los datos normalizados
X_norm_predic, mu, sigma = featureNormalize(X)

print('Media calculada:\n', mu)
print('Desviación estandar calculada:\n', sigma)
print('Datos normalizados:\n',X_norm_predic) # Muestra los datos normalizados

"""### 1.1 Visualizar los datos

Antes de comenzar a implementar cualquier algoritmo de aprendizaje, siempre es bueno visualizar los datos si es posible. Mostramos los datos en una gráfica bidimensional llamando a la función `plotData`. Se completará el código en `plotData` para que muestre una figura donde los ejes son los dos puntajes de los dos examenes, los ejemplos positivos y negativos se muestran con diferentes marcadores.
"""

#El 0 se convierte en False y El 1 se convierte en True
#y_bool = y != 0  # Convierte 0 a False y cualquier otro valor a True
def plotData(X_norm, y):
    # Gragica los puntos de datos X y y en una nueva figura. Grafica los puntos de datos con * para los positivos y
    # o para los negativos.

    # Crea una nueva figura
    fig = pyplot.figure()

    # Find Indices of Positive and Negative Examples
    pos = y == 1  # Identificar ejemplos positivos
    neg = y == 0  # Identificar ejemplos negativos

    # Plot Examples
    pyplot.plot(X_norm[pos, 0], X_norm[pos, 1], 'k*', lw=2, ms=10)  # Marcar los ejemplos positivos
    pyplot.plot(X_norm[neg, 0], X_norm[neg, 1], 'ko', mfc='y', ms=8, mec='k', mew=1)  # Marcar los ejemplos negativos
    return fig

"""Se llama a la función implementada para mostrar los datos cargados:"""

plotData(X_norm, y)
# adiciona etiquetas para los ejes
#Las líneas pyplot.xlabel() y pyplot.ylabel() etiquetan los ejes del gráfico, no las variables X e Y, entonces en el eje X va X0 y en el eje Y va X1
pyplot.xlabel('Activos Corrientes (Xo)')
pyplot.ylabel('Costo de los Bienes Vendidos (X1)')
pyplot.legend(['Alive ', 'Failed '])
pass

"""<a id="section1"></a>
### 1.2 Implementacion

#### 1.2.1 Fución Sigmoidea

La hipotesis para la regresión logistica se define como:

$$ h_\theta(x) = g(\theta^T x)$$

donde la función $g$ es la función sigmoidea. La función sigmoidea se define como:

$$g(z) = \frac{1}{1+e^{-z}}$$.

Los resultados que debe generar la funcion sigmoidea para valores positivos amplios de `x`, deben ser cercanos a 1, mientras que para valores negativos grandes, la sigmoide debe generar valores cercanos 0. La evaluacion de `sigmoid(0)` debe dar un resultado exacto de 0.5. Esta funcion tambien debe poder trabajar con vectores y matrices.
"""

def calcularSigmoide(z):
    # Calcula la sigmoide de una entrada z
    # convierte la intrada a un arreglo numpy
    z = np.array(z)

    g = np.zeros(z.shape)

    g = 1 / (1 + np.exp(-z))

    return g

"""Se calcula el valor de la sigmoide aplicando la funcion sigmoid con `z=0`, se debe obtener un resultado de 0.5. RE recomienda experimentar con otros valores de `z`."""

# Prueba la implementacion de la funcion sigmoid
z = [-9099991, 309000000, 0]
g = calcularSigmoide(z)

print('g(', z, ') = ', g)

"""<a id="section2"></a>
#### 1.2.2 Función de Costo y Gradiente

Se implementa la funcion cost y gradient, para la regresión logistica. Antes de continuar es importante agregar el termino de intercepcion a X.
"""

# Configurar la matriz adecuadamente, y agregar una columna de unos que corresponde al termino de intercepción.
m, n = X_norm.shape
# Agraga el termino de intercepción a A (sesgo)
X_norm = np.concatenate([np.ones((m, 1)), X_norm], axis=1)

print(X_norm[:])

"""La funcion de costo en una regresión logistica es:

$$ J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \left[ -y^{(i)} \log\left(h_\theta\left( x^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - h_\theta\left( x^{(i)} \right) \right) \right]$$

y el gradiente del costo es un vector de la misma longitud como $\theta$ donde el elemento $j^{th}$ (para $j = 0, 1, \cdots , n$) se define como:

$$ \frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^m \left( h_\theta \left( x^{(i)} \right) - y^{(i)} \right) x_j^{(i)} $$

Si bien este gradiente parece idéntico al gradiente de regresión lineal, la fórmula es diferente porque la regresión lineal y logística tienen diferentes definiciones de $h_\theta(x)$.
<a id="costFunction"></a>
"""

print("Dimensiones de X después de agregar el sesgo:", X_norm.shape)

def calcularCosto(theta, X_norm, y):
    m = y.size
    J = 0
    h = calcularSigmoide(X_norm.dot(theta))
    J = (1 / m) * np.sum(-y.dot(np.log(h + 1e-10)) - (1 - y).dot(np.log(1 - h + 1e-10)))
    #J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h)))
    return J

def descensoGradiente(theta, X_norm, y, alpha, num_iters):
    # Inicializa algunos valores
    m = y.shape[0] # numero de ejemplos de entrenamiento

    # realiza una copia de theta, el cual será acutalizada por el descenso por el gradiente
    theta = theta.copy()
    J_history = []

    for i in range(num_iters):
        h = calcularSigmoide(X_norm.dot(theta))
        theta = theta - (alpha / m) * (h - y).dot(X_norm)

        # Monitoreo del costo cada 100 iteraciones
        #if i % 100 == 0:
        #    costo = calcularCosto(theta, X_norm, y)
        #    print(f"Costo en la iteración {i}: {costo}")

        J_history.append(calcularCosto(theta, X_norm, y))
    return theta, J_history

"""### Nueva sección"""

# Elegir algun valor para alpha (probar varias alternativas)
alpha = 0.003
num_iters = 30000

# inicializa theta y ejecuta el descenso por el gradiente
theta = np.zeros(12) # son solo las 11 columnas de X más 1 que aumentamos de unos (sesgo)
theta, J_history = descensoGradiente(theta, X_norm, y, alpha, num_iters)

# Grafica la convergencia del costo
pyplot.plot(np.arange(len(J_history)), J_history)
pyplot.xlabel('Numero de iteraciones')
pyplot.ylabel('Costo J')
pyplot.show()

# Muestra los resultados del descenso por el gradiente
print('theta calculado por el descenso por el gradiente: {:s}'.format(str(theta)))


print('Theta encontrada por descenso gradiente: {:.4f}, {:.4f}'.format(*theta))
print(f"Costo final después de {num_iters} iteraciones: {J_history[-1]:.2f}")

print(J_history[-1])

for i in range(1,10):
  prob = calcularSigmoide(np.dot([random.uniform(0,1) for _ in range(12)], theta))
  print(i,'probabilidad de quiebra de empresas: {:.3f}%'.format(prob))

"""### Calculamos la precision del entrenamiento"""

def predict(theta, X_norm):
    m = X_norm.shape[0]
    p = np.zeros(m)
    p = np.round(calcularSigmoide(X_norm.dot(theta.reshape(-1,1)))) # Redimensiona theta para ser un vector columna
    return p

p = predict(theta, X_norm) # Usa X_norm en lugar de X como entrada
print('Precisión de entrenamiento: {:.2f} %'.format(np.mean(p == y) * 100))

#imprimir theta
print('Theta encontrada por descenso gradiente: {:.4f}, {:.4f}'.format(*theta))